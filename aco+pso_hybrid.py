# -*- coding: utf-8 -*-
"""ACO+PSO hybrid

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SIlk267s-5Xd_m1Ct4zYLEMOqN1PK2iu
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import time

class HybridFeatureSelection:
    def _init_(self, columns_list, data_dict, model_aco, model_pso, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', aco_iterations=100, aco_n_ants=100, pso_iterations=100, pso_n_particles=30, aco_evaporation_rate=0.9, aco_Q=0.2, pso_w=0.5, pso_c1=0.8, pso_c2=0.9):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.model_aco = model_aco
        self.model_pso = model_pso
        self.cost_function = cost_function
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.cost_function_improvement = cost_function_improvement
        self.aco_iterations = aco_iterations
        self.aco_n_ants = aco_n_ants
        self.pso_iterations = pso_iterations
        self.pso_n_particles = pso_n_particles
        self.aco_evaporation_rate = aco_evaporation_rate
        self.aco_Q = aco_Q
        self.pso_w = pso_w
        self.pso_c1 = pso_c1
        self.pso_c2 = pso_c2

        self.aco_fp = [1]*(len(columns_list))
        self.aco_ants = []
        self.aco_size = len(columns_list)
        self.aco_top_score = 0
        self.aco_result = []

        self.pso_dimension = len(columns_list)
        self.pso_particles_position = np.random.rand(self.pso_n_particles, self.pso_dimension) > 0.5
        self.pso_particles_velocity = np.zeros((self.pso_n_particles, self.pso_dimension))
        self.pso_pbest_position = self.pso_particles_position.copy()
        self.pso_gbest_position = self.pso_particles_position[0].copy()
        self.pso_pbest_value = np.array([float('inf')] * self.pso_n_particles)
        self.pso_gbest_value = float('inf')


    def run_optimization(self):
        # Add your optimization logic here
        best_features = []  # Placeholder for best features
        best_score = 0  # Placeholder for best score

        # Perform your hybrid optimization using ACO and PSO
        # Update best_features and best_score accordingly

        return best_features, best_score

    # ACO methods...
    # Define the Ant Colony Optimization (ACO) feature selection class
class AntColonyOptimizationFS:
    def _init_(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=100, N_ants=100, print_iteration_result=True, evaporation_rate=0.9, Q=0.2):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.model = model
        self.cost_function = cost_function
        self.cost_function_improvement = cost_function_improvement
        self.iterations = iterations
        self.N_ants = N_ants
        self.print_iteration_result = print_iteration_result
        self.evaporation_rate = evaporation_rate
        self.Q = Q

        self.fp = [1] * (len(columns_list))
        self.ants = []
        self.size = len(columns_list)
        self.topScore = 0
        self.result = []

    # PSO methods...
    # Define the Particle Swarm Optimization (PSO) feature selection class
class ParticleSwarmOptimizationFS:
    def _init_(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=100, n_particles=30, w=0.5, c1=0.8, c2=0.9):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.model = model
        self.cost_function = cost_function
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.cost_function_improvement = cost_function_improvement
        self.iterations = iterations
        self.n_particles = n_particles
        self.w = w
        self.c1 = c1
        self.c2 = c2

        self.particles_position = np.random.rand(self.n_particles, len(columns_list)) > 0.5
        self.particles_velocity = np.zeros((self.n_particles, len(columns_list)))
        self.pbest_position = self.particles_position.copy()
        self.gbest_position = self.particles_position[0].copy()
        self.pbest_value = np.array([float('inf')] * self.n_particles)
        self.gbest_value = float('inf')
        self.dimension = len(columns_list)


    def _calculate_aco_cost(self, current_at_feature_subset):
        fold_cost = []

        for i in self.data_dict.keys():
            x_train = self.data_dict[i]['x_train'][current_at_feature_subset]
            self.model_aco.fit(x_train)
            y_test_predict = self.model_aco.predict(x_train)
            fold_cost.append(self.cost_function(y_test, y_test_predict))

        return np.mean(fold_cost)

    def _calculate_pso_cost(self, particle_position):
        selected_features = [self.columns_list[i] for i in range(self.pso_dimension) if particle_position[i]]
        if not selected_features:
            return float('inf')

        fold_cost = []
        for i in self.data_dict.keys():
            x_train = self.data_dict[i]['x_train'][selected_features]
            self.model_pso.fit(x_train)
            y_test_predict = self.model_pso.predict(x_train)
            fold_cost.append(self.cost_function(y_test, y_test_predict))

        return np.mean(fold_cost)

    # Hybrid feature selection methods...

    def get_hybrid_features(self):
        # ACO and PSO feature selection...
        return hybrid_features

# Load your dataset from a CSV file
data = pd.read_csv("/content/drive/MyDrive/Breast Cancer DataSET/haralick_features/Haralick_normal_10_0.csv")

# Extract all column names
columns_list = data.columns.tolist()

# Split the data into training, testing, and validation sets
X_train, X_temp = train_test_split(data, test_size=0.2, random_state=42)
X_test, X_validation = train_test_split(X_temp, test_size=0.5, random_state=42)

# Now you can create your data dictionary
data_dict = {
    0: {'x_train': X_train, 'y_train': None, 'x_test': X_test, 'y_test': None},
    # You can add more keys for cross-validation data if needed
}

# Initialize your models
model_aco = RandomForestClassifier()
model_pso = RandomForestClassifier()

# Define your cost function (e.g., accuracy)
def cost_function(y_true, y_pred):
    return accuracy_score(y_true, y_pred)

# Instantiate the HybridFeatureSelection class
hybrid_fs = HybridFeatureSelection(columns_list=columns_list,
                                   data_dict=data_dict,
                                   model_aco=model_aco,
                                   model_pso=model_pso,
                                   cost_function=cost_function,
                                   x_validation_dataframe=X_validation,
                                   y_validation_dataframe=None,  # No target variable for validation
                                   use_validation_data=True,
                                   cost_function_improvement='increase',
                                   aco_iterations=100,
                                   aco_n_ants=100,
                                   pso_iterations=100,
                                   pso_n_particles=30,
                                   aco_evaporation_rate=0.9,
                                   aco_Q=0.2,
                                   pso_w=0.5,
                                   pso_c1=0.8,
                                   pso_c2=0.9)

# Run the optimization
start_time = time.time()
best_features, best_score = hybrid_fs.run_optimization()
end_time = time.time()

print("Best features selected:", best_features)
print("Best score achieved:", best_score)
print("Execution time:", (end_time - start_time), "seconds")

pip install pyswarm

import numpy as np
import pandas as pd
from pyswarm import pso
from functools import partial

# Load your dataset from a CSV file
data = pd.read_csv("/content/drive/MyDrive/Breast Cancer DataSET/haralick_features/Haralick_normal_10_0.csv")

# Assuming all columns are features and there is no target column
columns_list = data.columns.tolist()

# Define your cost function for ACO
# Define your cost function for ACO
def cost_function_aco(features, data):
    # Select the subset of features
    selected_features = [columns_list[i] for i, feature in enumerate(features) if feature == 1]
    if len(selected_features) == 0:
        return -np.inf  # Return a low score if no features selected

    # Use some unsupervised metric to evaluate the selected features
    # Example: Sum of selected feature values
    selected_data = data[selected_features]
    score = np.sum(selected_data.values)  # Convert the Pandas DataFrame to a NumPy array using .values

    return -score  # We want to maximize the unsupervised metric, so negate the score

# Define the bounds for PSO
lower_bound = np.zeros(len(columns_list))
upper_bound = np.ones(len(columns_list))

# Define the objective function for PSO
objective_function_aco = partial(cost_function_aco, data=data)

# Perform PSO to select features for ACO
best_features_aco, _ = pso(objective_function_aco, lower_bound, upper_bound, swarmsize=100, maxiter=100)

# Convert the best_features array to binary (0 or 1)
best_features_binary_aco = [1 if feature > 0.5 else 0 for feature in best_features_aco]

# Select the final best features for ACO
final_features_aco = [columns_list[i] for i, feature in enumerate(best_features_binary_aco) if feature == 1]

# Now you can use the selected features for ACO in PSO
# Define your cost function for PSO using the selected features from ACO

# Define your cost function for PSO
# Define your cost function for PSO
def cost_function_pso(features, data, selected_features_aco):
    # Select the subset of features
    selected_features = [selected_features_aco[i] for i, feature in enumerate(features) if feature == 1]
    if len(selected_features) == 0:
        return -np.inf  # Return a low score if no features selected

    # Use some unsupervised metric to evaluate the selected features
    # Example: Sum of selected feature values
    selected_data = data[selected_features]
    score = np.sum(selected_data.values)  # Convert the Pandas DataFrame to a NumPy array using .values

    return -score  # We want to maximize the unsupervised metric, so negate the score

# Define the objective function for PSO
objective_function_pso = partial(cost_function_pso, data=data)

# # Perform PSO to select features for PSO using the features selected by ACO
# best_features_pso, _ = pso(objective_function_pso, lower_bound, upper_bound, args=(final_features_aco, data), swarmsize=100, maxiter=100)




# Convert the best_features array to binary (0 or 1)
best_features_binary_pso = [1 if feature > 0.5 else 0 for feature in best_features_aco]

# Select the final best features for PSO
final_features_pso = [final_features_aco[i] for i, feature in enumerate(best_features_binary_pso) if feature == 1 and i < len(final_features_aco)]

print("Best features selected by ACO:", final_features_aco)
print("Best features selected by PSO using features selected by ACO:", final_features_pso)

import numpy as np
import pandas as pd
from pyswarm import pso
from functools import partial

# Load your dataset from a CSV file
data = pd.read_csv("/content/drive/MyDrive/Breast Cancer DataSET/haralick_features/Haralick_normal_15.csv")

# Assuming all columns are features and there is no target column
columns_list = data.columns.tolist()

# Define your cost function for ACO
# Define your cost function for ACO
def cost_function_aco(features, data):
    # Select the subset of features
    selected_features = [columns_list[i] for i, feature in enumerate(features) if feature == 1]
    if len(selected_features) == 0:
        return -np.inf  # Return a low score if no features selected

    # Use some unsupervised metric to evaluate the selected features
    # Example: Sum of selected feature values
    selected_data = data[selected_features]
    score = np.sum(selected_data.values)  # Convert the Pandas DataFrame to a NumPy array using .values

    return -score  # We want to maximize the unsupervised metric, so negate the score

# Define the bounds for PSO
lower_bound = np.zeros(len(columns_list))
upper_bound = np.ones(len(columns_list))

# Define the objective function for PSO
objective_function_aco = partial(cost_function_aco, data=data)

# Perform PSO to select features for ACO
best_features_aco, _ = pso(objective_function_aco, lower_bound, upper_bound, swarmsize=100, maxiter=100)

# Convert the best_features array to binary (0 or 1)
best_features_binary_aco = [1 if feature > 0.5 else 0 for feature in best_features_aco]

# Select the final best features for ACO
final_features_aco = [columns_list[i] for i, feature in enumerate(best_features_binary_aco) if feature == 1]

# Now you can use the selected features for ACO in PSO
# Define your cost function for PSO using the selected features from ACO

# Define your cost function for PSO
# Define your cost function for PSO
def cost_function_pso(features, data, selected_features_aco):
    # Select the subset of features
    selected_features = [selected_features_aco[i] for i, feature in enumerate(features) if feature == 1]
    if len(selected_features) == 0:
        return -np.inf  # Return a low score if no features selected

    # Use some unsupervised metric to evaluate the selected features
    # Example: Sum of selected feature values
    selected_data = data[selected_features]
    score = np.sum(selected_data.values)  # Convert the Pandas DataFrame to a NumPy array using .values

    return -score  # We want to maximize the unsupervised metric, so negate the score

# Define the objective function for PSO
objective_function_pso = partial(cost_function_pso, data=data)

# # Perform PSO to select features for PSO using the features selected by ACO
# best_features_pso, _ = pso(objective_function_pso, lower_bound, upper_bound, args=(final_features_aco, data), swarmsize=100, maxiter=100)




# Convert the best_features array to binary (0 or 1)
best_features_binary_pso = [1 if feature > 0.5 else 0 for feature in best_features_aco]

# Select the final best features for PSO
final_features_pso = [final_features_aco[i] for i, feature in enumerate(best_features_binary_pso) if feature == 1 and i < len(final_features_aco)]

print("Best features selected by ACO:", final_features_aco)
print("Best features selected by PSO using features selected by ACO:", final_features_pso)

from google.colab import drive
drive.mount('/content/drive')

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming 'target' is your target variable column
X_train, X_test, y_train, y_test = train_test_split(data[final_features_pso], data['target'], test_size=0.2, random_state=42)

# Initialize and train a classifier (e.g., RandomForestClassifier)
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming 'target' is your target variable column
X_train, X_test, y_train, y_test = train_test_split(data[final_features_pso], data['target'], test_size=0.2, random_state=42)

# Initialize and train a classifier (e.g., RandomForestClassifier)
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import time



    def run_optimization(self):
        # Implementation of ACO optimization here

        pass

class ParticleSwarmOptimizationFS:
    def __init__(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=100, n_particles=30, w=0.5, c1=0.8, c2=0.9):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.model = model
        self.cost_function = cost_function
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.cost_function_improvement = cost_function_improvement
        self.iterations = iterations
        self.n_particles = n_particles
        self.w = w
        self.c1 = c1
        self.c2 = c2

        self.particles_position = np.random.rand(self.n_particles, len(columns_list)) > 0.5
        self.particles_velocity = np.zeros((self.n_particles, len(columns_list)))
        self.pbest_position = self.particles_position.copy()
        self.gbest_position = self.particles_position[0].copy()
        self.pbest_value = np.array([float('inf')] * self.n_particles)
        self.gbest_value = float('inf')
        self.dimension = len(columns_list)

    def run_optimization(self):
        # Implementation of PSO optimization here
        pass

class HybridFeatureSelection:
    def __init__(self, columns_list, data_dict, model_aco, model_pso, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', aco_iterations=100, aco_n_ants=100, pso_iterations=100, pso_n_particles=30, aco_evaporation_rate=0.9, aco_Q=0.2, pso_w=0.5, pso_c1=0.8, pso_c2=0.9):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.model_aco = model_aco
        self.model_pso = model_pso
        self.cost_function = cost_function
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.cost_function_improvement = cost_function_improvement
        self.aco_iterations = aco_iterations
        self.aco_n_ants = aco_n_ants
        self.pso_iterations = pso_iterations
        self.pso_n_particles = pso_n_particles
        self.aco_evaporation_rate = aco_evaporation_rate
        self.aco_Q = aco_Q
        self.pso_w = pso_w
        self.pso_c1 = pso_c1
        self.pso_c2 = pso_c2

        self.aco_fp = [1] * (len(columns_list))
        self.aco_ants = []
        self.aco_size = len(columns_list)
        self.aco_top_score = 0
        self.aco_result = []

        self.pso_dimension = len(columns_list)
        self.pso_particles_position = np.random.rand(self.pso_n_particles, self.pso_dimension) > 0.5
        self.pso_particles_velocity = np.zeros((self.pso_n_particles, self.pso_dimension))
        self.pso_pbest_position = self.pso_particles_position.copy()
        self.pso_gbest_position = self.pso_particles_position[0].copy()
        self.pso_pbest_value = np.array([float('inf')] * self.pso_n_particles)
        self.pso_gbest_value = float('inf')

    def run_optimization(self):
      aco = AntColonyOptimizationFS(columns_list=self.columns_list,
                                    data_dict=self.data_dict,
                                    model=self.model_aco,
                                    cost_function=self.cost_function,
                                    iterations=self.aco_iterations,
                                    N_ants=self.aco_n_ants,
                                    evaporation_rate=self.aco_evaporation_rate,
                                    Q=self.aco_Q)
      aco_best_features, aco_best_score = aco.run_optimization()

      pso = ParticleSwarmOptimizationFS(columns_list=self.columns_list,
                                        data_dict=self.data_dict,
                                        model=self.model_pso,
                                        cost_function=self.cost_function,
                                        iterations=self.pso_iterations,
                                        n_particles=self.pso_n_particles,
                                        w=self.pso_w,
                                        c1=self.pso_c1,
                                        c2=self.pso_c2)
      pso_best_features, pso_best_score = pso.run_optimization()

      # Compare and select the best feature subset
      if aco_best_score > pso_best_score:
          return aco_best_features, aco_best_score
      else:
          return pso_best_features, pso_best_score




# Example usage
data = pd.read_csv("/content/drive/MyDrive/Breast Cancer DataSET/haralick_features/Haralick_normal_10_0.csv")
columns_list = data.columns.tolist()[:-1]

X_train, X_temp = train_test_split(data, test_size=0.2, random_state=42)
X_test, X_validation = train_test_split(X_temp, test_size=0.5, random_state=42)

data_dict = {
    0: {'x_train': X_train, 'y_train': None, 'x_test': X_test, 'y_test': None},
}

model_aco = RandomForestClassifier()
model_pso = RandomForestClassifier()

def cost_function(y_true, y_pred):
    return accuracy_score(y_true, y_pred)

hybrid_fs = HybridFeatureSelection(columns_list=columns_list,
                                   data_dict=data_dict,
                                   model_aco=model_aco,
                                   model_pso=model_pso,
                                   cost_function=cost_function,
                                   x_validation_dataframe=X_validation,
                                   y_validation_dataframe=None,
                                   use_validation_data=True,
                                   cost_function_improvement='increase',
                                   aco_iterations=100,
                                   aco_n_ants=100,
                                   pso_iterations=100,
                                   pso_n_particles=30,
                                   aco_evaporation_rate=0.9,
                                   aco_Q=0.2,
                                   pso_w=0.5,
                                   pso_c1=0.8,
                                   pso_c2=0.9)

start_time = time.time()
best_features, best_score = hybrid_fs.run_optimization()
end_time = time.time()

print("Best features selected:", best_features)
print("Best score achieved:", best_score)
print("Execution time:", (end_time - start_time), "seconds")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import time

class AntColonyOptimizationFS:
    def __init__(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=100, N_ants=100, print_iteration_result=True, evaporation_rate=0.9, Q=0.2):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.model = model
        self.cost_function = cost_function
        self.cost_function_improvement = cost_function_improvement
        self.iterations = iterations
        self.N_ants = N_ants
        self.print_iteration_result = print_iteration_result
        self.evaporation_rate = evaporation_rate
        self.Q = Q

        self.fp = [1] * (len(columns_list))
        self.ants = []
        self.size = len(columns_list)
        self.topScore = 0
        self.result = []

    def run_optimization(self):
        # Implementation of ACO optimization here
        pass

class ParticleSwarmOptimizationFS:
    def __init__(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=100, n_particles=30, w=0.5, c1=0.8, c2=0.9):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.model = model
        self.cost_function = cost_function
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.cost_function_improvement = cost_function_improvement
        self.iterations = iterations
        self.n_particles = n_particles
        self.w = w
        self.c1 = c1
        self.c2 = c2

        self.particles_position = np.random.rand(self.n_particles, len(columns_list)) > 0.5
        self.particles_velocity = np.zeros((self.n_particles, len(columns_list)))
        self.pbest_position = self.particles_position.copy()
        self.gbest_position = self.particles_position[0].copy()
        self.pbest_value = np.array([float('inf')] * self.n_particles)
        self.gbest_value = float('inf')
        self.dimension = len(columns_list)

    def run_optimization(self):
        # Implementation of PSO optimization here
        pass

class HybridFeatureSelection:
    def __init__(self, columns_list, data_dict, model_aco, model_pso, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', aco_iterations=100, aco_n_ants=100, pso_iterations=100, pso_n_particles=30, aco_evaporation_rate=0.9, aco_Q=0.2, pso_w=0.5, pso_c1=0.8, pso_c2=0.9):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.model_aco = model_aco
        self.model_pso = model_pso
        self.cost_function = cost_function
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.cost_function_improvement = cost_function_improvement
        self.aco_iterations = aco_iterations
        self.aco_n_ants = aco_n_ants
        self.pso_iterations = pso_iterations
        self.pso_n_particles = pso_n_particles
        self.aco_evaporation_rate = aco_evaporation_rate
        self.aco_Q = aco_Q
        self.pso_w = pso_w
        self.pso_c1 = pso_c1
        self.pso_c2 = pso_c2

        self.aco_fp = [1] * (len(columns_list))
        self.aco_ants = []
        self.aco_size = len(columns_list)
        self.aco_top_score = 0
        self.aco_result = []

        self.pso_dimension = len(columns_list)
        self.pso_particles_position = np.random.rand(self.pso_n_particles, self.pso_dimension) > 0.5
        self.pso_particles_velocity = np.zeros((self.pso_n_particles, self.pso_dimension))
        self.pso_pbest_position = self.pso_particles_position.copy()
        self.pso_gbest_position = self.pso_particles_position[0].copy()
        self.pso_pbest_value = np.array([float('inf')] * self.pso_n_particles)
        self.pso_gbest_value = float('inf')

    def run_optimization(self):
      aco = AntColonyOptimizationFS(columns_list=self.columns_list,
                                    data_dict=self.data_dict,
                                    model=self.model_aco,
                                    cost_function=self.cost_function,
                                    iterations=self.aco_iterations,
                                    N_ants=self.aco_n_ants,
                                    evaporation_rate=self.aco_evaporation_rate,
                                    Q=self.aco_Q)
      aco_best_features, aco_best_score = aco.run_optimization()

      pso = ParticleSwarmOptimizationFS(columns_list=self.columns_list,
                                        data_dict=self.data_dict,
                                        model=self.model_pso,
                                        cost_function=self.cost_function,
                                        iterations=self.pso_iterations,
                                        n_particles=self.pso_n_particles,
                                        w=self.pso_w,
                                        c1=self.pso_c1,
                                        c2=self.pso_c2)
      pso_best_features, pso_best_score = pso.run_optimization()

      # Compare and select the best feature subset
      if aco_best_score > pso_best_score:
          return aco_best_features, aco_best_score
      else:
          return pso_best_features, pso_best_score




# Example usage
data = pd.read_csv("/content/drive/MyDrive/Breast Cancer DataSET/haralick_features/Haralick_normal_10_0.csv")
columns_list = data.columns.tolist()[:-1]

X_train, X_temp = train_test_split(data, test_size=0.2, random_state=42)
X_test, X_validation = train_test_split(X_temp, test_size=0.5, random_state=42)

data_dict = {
    0: {'x_train': X_train, 'y_train': None, 'x_test': X_test, 'y_test': None},
}

model_aco = RandomForestClassifier()
model_pso = RandomForestClassifier()

def cost_function(y_true, y_pred):
    return accuracy_score(y_true, y_pred)

hybrid_fs = HybridFeatureSelection(columns_list=columns_list,
                                   data_dict=data_dict,
                                   model_aco=model_aco,
                                   model_pso=model_pso,
                                   cost_function=cost_function,
                                   x_validation_dataframe=X_validation,
                                   y_validation_dataframe=None,
                                   use_validation_data=True,
                                   cost_function_improvement='increase',
                                   aco_iterations=100,
                                   aco_n_ants=100,
                                   pso_iterations=100,
                                   pso_n_particles=30,
                                   aco_evaporation_rate=0.9,
                                   aco_Q=0.2,
                                   pso_w=0.5,
                                   pso_c1=0.8,
                                   pso_c2=0.9)

start_time = time.time()
best_features, best_score = hybrid_fs.run_optimization()
end_time = time.time()

print("Best features selected:", best_features)
print("Best score achieved:", best_score)
print("Execution time:", (end_time - start_time), "seconds")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import time

class HybridFeatureSelection:
    def __init__(self, columns_list, data_dict, model_aco, model_pso, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', aco_iterations=100, aco_n_ants=100, pso_iterations=100, pso_n_particles=30, aco_evaporation_rate=0.9, aco_Q=0.2, pso_w=0.5, pso_c1=0.8, pso_c2=0.9):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.model_aco = model_aco
        self.model_pso = model_pso
        self.cost_function = cost_function
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.cost_function_improvement = cost_function_improvement
        self.aco_iterations = aco_iterations
        self.aco_n_ants = aco_n_ants
        self.pso_iterations = pso_iterations
        self.pso_n_particles = pso_n_particles
        self.aco_evaporation_rate = aco_evaporation_rate
        self.aco_Q = aco_Q
        self.pso_w = pso_w
        self.pso_c1 = pso_c1
        self.pso_c2 = pso_c2

        self.aco_fp = [1]*(len(columns_list))
        self.aco_ants = []
        self.aco_size = len(columns_list)
        self.aco_top_score = 0
        self.aco_result = []

        self.pso_dimension = len(columns_list)
        self.pso_particles_position = np.random.rand(self.pso_n_particles, self.pso_dimension) > 0.5
        self.pso_particles_velocity = np.zeros((self.pso_n_particles, self.pso_dimension))
        self.pso_pbest_position = self.pso_particles_position.copy()
        self.pso_gbest_position = self.pso_particles_position[0].copy()
        self.pso_pbest_value = np.array([float('inf')] * self.pso_n_particles)
        self.pso_gbest_value = float('inf')

    # ACO methods...
    # Define the Ant Colony Optimization (ACO) feature selection class
class AntColonyOptimizationFS:
    def __init__(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=100, N_ants=100, print_iteration_result=True, evaporation_rate=0.9, Q=0.2):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.model = model
        self.cost_function = cost_function
        self.cost_function_improvement = cost_function_improvement
        self.iterations = iterations
        self.N_ants = N_ants
        self.print_iteration_result = print_iteration_result
        self.evaporation_rate = evaporation_rate
        self.Q = Q

        self.fp = [1] * (len(columns_list))
        self.ants = []
        self.size = len(columns_list)
        self.topScore = 0
        self.result = []

    # PSO methods...
    # Define the Particle Swarm Optimization (PSO) feature selection class
class ParticleSwarmOptimizationFS:
    def __init__(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=100, n_particles=30, w=0.5, c1=0.8, c2=0.9):
        self.columns_list = columns_list
        self.data_dict = data_dict
        self.model = model
        self.cost_function = cost_function
        self.x_validation_dataframe = x_validation_dataframe
        self.y_validation_dataframe = y_validation_dataframe
        self.use_validation_data = use_validation_data
        self.cost_function_improvement = cost_function_improvement
        self.iterations = iterations
        self.n_particles = n_particles
        self.w = w
        self.c1 = c1
        self.c2 = c2

        self.particles_position = np.random.rand(self.n_particles, len(columns_list)) > 0.5
        self.particles_velocity = np.zeros((self.n_particles, len(columns_list)))
        self.pbest_position = self.particles_position.copy()
        self.gbest_position = self.particles_position[0].copy()
        self.pbest_value = np.array([float('inf')] * self.n_particles)
        self.gbest_value = float('inf')
        self.dimension = len(columns_list)


    def _calculate_aco_cost(self, current_at_feature_subset):
        fold_cost = []

        for i in self.data_dict.keys():
            x_train = self.data_dict[i]['x_train'][current_at_feature_subset]
            self.model_aco.fit(x_train)
            y_test_predict = self.model_aco.predict(x_train)
            fold_cost.append(self.cost_function(y_test, y_test_predict))

        return np.mean(fold_cost)

    def _calculate_pso_cost(self, particle_position):
        selected_features = [self.columns_list[i] for i in range(self.pso_dimension) if particle_position[i]]
        if not selected_features:
            return float('inf')

        fold_cost = []
        for i in self.data_dict.keys():
            x_train = self.data_dict[i]['x_train'][selected_features]
            self.model_pso.fit(x_train)
            y_test_predict = self.model_pso.predict(x_train)
            fold_cost.append(self.cost_function(y_test, y_test_predict))

        return np.mean(fold_cost)

    # Hybrid feature selection methods...

    def get_hybrid_features(self):
        # ACO and PSO feature selection...
        return hybrid_features

# Load your dataset from a CSV file
data = pd.read_csv("/content/drive/MyDrive/Breast Cancer DataSET/haralick_features/Haralick_normal_10_0.csv")

# Extract all column names (excluding the last column)
columns_list = data.columns.tolist()[:-1]

# Assuming 'target' column is your target variable
# X = data.drop(columns=['target'])  # Features
# y = data['target']  # Target variable

# Split the data into training, testing, and validation sets
X_train, X_temp = train_test_split(data, test_size=0.2, random_state=42)
X_test, X_validation = train_test_split(X_temp, test_size=0.5, random_state=42)

# Now you can create your data dictionary
data_dict = {
    0: {'x_train': X_train.drop(columns=['target']), 'y_train': None, 'x_test': X_test.drop(columns=['target']), 'y_test': None},
    # You can add more keys for cross-validation data if needed
}

# Initialize your models
model_aco = RandomForestClassifier()
model_pso = RandomForestClassifier()

# Define your cost function (e.g., accuracy)
def cost_function(y_true, y_pred):
    return accuracy_score(y_true, y_pred)

# Instantiate the HybridFeatureSelection class
hybrid_fs = HybridFeatureSelection(columns_list=columns_list,
                                   data_dict=data_dict,
                                   model_aco=model_aco,
                                   model_pso=model_pso,
                                   cost_function=cost_function,
                                   x_validation_dataframe=X_validation.drop(columns=['target']),
                                   y_validation_dataframe=None,  # No target variable for validation
                                   use_validation_data=True,
                                   cost_function_improvement='increase',
                                   aco_iterations=100,
                                   aco_n_ants=100,
                                   pso_iterations=100,
                                   pso_n_particles=30,
                                   aco_evaporation_rate=0.9,
                                   aco_Q=0.2,
                                   pso_w=0.5,
                                   pso_c1=0.8,
                                   pso_c2=0.9)

# Run the optimization
start_time = time.time()
best_features, best_score = hybrid_fs.run_optimization()
end_time = time.time()

print("Best features selected:", best_features)
print("Best score achieved:", best_score)
print("Execution time:", (end_time - start_time), "seconds")

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Generating a dataset for demonstration
X, y = make_classification(n_samples=1000, n_features=30, n_informative=15, n_redundant=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

n_features = X_train.shape[1]

# PSO Parameters
n_particles = 10
max_iterations = 20
inertia_weight = 0.5
cognitive_weight = 1.5
social_weight = 1.5

# ACO Parameters
n_ants = 10
n_iterations = 10
evaporation_rate = 0.5
alpha = 1
beta = 1

# Initialize PSO particles
particle_positions = np.random.randint(2, size=(n_particles, n_features))
particle_velocities = np.zeros((n_particles, n_features))
particle_best_positions = np.copy(particle_positions)
particle_best_scores = np.array([float('-inf')] * n_particles)
global_best_position = np.zeros(n_features)
global_best_score = float('-inf')

def fitness_function(features):
    if np.sum(features) == 0:
        return 0
    clf = RandomForestClassifier()
    clf.fit(X_train[:, features == 1], y_train)
    y_pred = clf.predict(X_test[:, features == 1])
    return accuracy_score(y_test, y_pred)

# Hybrid PSO-ACO optimization loop
for iteration in range(max_iterations):
    # PSO Part
    for i in range(n_particles):
        fitness = fitness_function(particle_positions[i])
        if fitness > particle_best_scores[i]:
            particle_best_scores[i] = fitness
            particle_best_positions[i] = particle_positions[i].copy()
        if fitness > global_best_score:
            global_best_score = fitness
            global_best_position = particle_positions[i].copy()

    for i in range(n_particles):
        particle_velocities[i] = inertia_weight * particle_velocities[i] + \
                                 cognitive_weight * np.random.rand() * (particle_best_positions[i] - particle_positions[i]) + \
                                 social_weight * np.random.rand() * (global_best_position - particle_positions[i])
        particle_positions[i] += particle_velocities[i] > 0
        particle_positions[i] = np.clip(particle_positions[i], 0, 1)

    # ACO Part (Local Search around the global best position found by PSO)
    # Here, we assume an ant modifies the global_best_position by flipping a bit to explore locally
    for ant in range(n_ants):
        candidate_position = global_best_position.copy()
        feature_to_flip = np.random.randint(n_features)
        candidate_position[feature_to_flip] = 1 - candidate_position[feature_to_flip]  # Flip the feature bit
        fitness = fitness_function(candidate_position)
        if fitness > global_best_score:
            global_best_score = fitness
            global_best_position = candidate_position.copy()

print(f"Global Best Score: {global_best_score}")
print(f"Global Best Position: {global_best_position}")
print(f"Selected Features: {np.where(global_best_position == 1)[0]}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import time
from copy import deepcopy
import pandas as pd
import numpy as np

class AntColonyOptimizationFS:

    '''
    Machine Learning Parameters
    ----------

    columns_list : Column names present in x_train_dataframe and x_test which will be used as input list for searching best list of features.

    data_dict : X and Y training and test data provided in dictionary format. Below is example of 5 fold cross validation data with keys.
        {0:{'x_train':x_train_dataframe,'y_train':y_train_array,'x_test':x_test_dataframe,'y_test':y_test_array},
        1:{'x_train':x_train_dataframe,'y_train':y_train_array,'x_test':x_test_dataframe,'y_test':y_test_array},
        2:{'x_train':x_train_dataframe,'y_train':y_train_array,'x_test':x_test_dataframe,'y_test':y_test_array},
        3:{'x_train':x_train_dataframe,'y_train':y_train_array,'x_test':x_test_dataframe,'y_test':y_test_array},
        4:{'x_train':x_train_dataframe,'y_train':y_train_array,'x_test':x_test_dataframe,'y_test':y_test_array}}

        If you only have train and test data and do not wish to do cross validation, use above dictionary format, with only one key.

    use_validation_data : Whether you want to use validation data as a boolean True or False. Default value is True. If false, user need not provide x_validation_dataframe and y_validation_dataframe

    x_validation_dataframe : dataframe containing features of validatoin dataset

    y_validation_dataframe : dataframe containing dependent variable of validation dataset

    model : Model object. It should have .fit and .predict attribute

    cost_function_improvement : Objective is to whether increase or decrease the cost during subsequent iterations.
        For regression it should be 'decrease' and for classification it should be 'increase'

    cost_function : Cost function for finding cost between actual and predicted values, depending on regression or classification problem.
        cost function should accept 'actual' and 'predicted' as arrays and return cost for the both.

    average : Averaging to be used. This is useful for clasification metrics such as 'f1_score', 'jaccard_score', 'fbeta_score', 'precision_score',
        'recall_score' and 'roc_auc_score' when dependent variable is multi-class

    Ant Colony Optimization Parameters
    ----------

    iterations : Number of times ant colony optimization will search for solutions. Default is 100

    N_ants : Number of ants in each iteration. Default is 100.

    run_time : Number of minutes to run the algorithm. This is checked in between each iteration.
        At start of each generation it is checked if runtime has exceeded than alloted time.
        If case run time did exceeds provided limit, best result from iterations executed so far is given as output.
        Default is 2 hours. i.e. 120 minutes.

    evaporation_rate : Evaporation rate. Values are between 0 and 1. If it is too large, chances are higher to find global optima, but computationally expensive. If it is low, chances of finding global optima are less. Default is kept as 0.9

    Q : Pheromene update coefficient. Value between 0 and 1. It affects the convergence speed. If it is large, ACO will get stuck at local optima. Default is kept as 0.2

    Output
    ----------
    best_columns : List object with list of column names which gives best performance for the model. These features can be used for training and saving models separately by the user.

    '''

    def _init_(self,columns_list,data_dict,model,cost_function,x_validation_dataframe=pd.DataFrame(),y_validation_dataframe=pd.DataFrame(),use_validation_data=True,cost_function_improvement='increase',average=None,iterations=100,N_ants=100, print_iteration_result = True,evaporation_rate=0.9,Q=0.2):
        self.columns_list=columns_list
        self.data_dict=data_dict
        self.x_validation_dataframe=x_validation_dataframe
        self.y_validation_dataframe=y_validation_dataframe
        self.use_validation_data=use_validation_data
        self.model=model
        self.cost_function=cost_function
        self.cost_function_improvement=cost_function_improvement
        self.average=average
        self.iterations = iterations
        self.N_ants = N_ants
        self.print_iteration_result = print_iteration_result
        self.evaporation_rate = evaporation_rate
        self.Q = Q

        self.fp = [1]*(len(columns_list))
        self.ants = []
        self.size = len(columns_list)
        self.topScore = 0
        self.result=[]

    def _calculate_cost(self,current_at_feature_subset):

        fold_cost = []

        for i in self.data_dict.keys():

            x_train=self.data_dict[i]['x_train'][current_at_feature_subset]
            y_train=self.data_dict[i]['y_train']

            x_test=self.data_dict[i]['x_test'][current_at_feature_subset]
            y_test=self.data_dict[i]['y_test']

            self.model.fit(x_train,y_train)

            y_test_predict=self.model.predict(x_test)

            if self.use_validation_data:
                y_validation_predict=self.model.predict(self.x_validation_dataframe[current_at_feature_subset])

            if self.average:
                fold_cost.append(self.cost_function(y_test,y_test_predict,average=self.average))
                fold_cost.append(self.cost_function(self.y_validation_dataframe,y_validation_predict,average=self.average))
            else:
                fold_cost.append(self.cost_function(y_test,y_test_predict))
                fold_cost.append(self.cost_function(self.y_validation_dataframe,y_validation_predict))

        return np.mean(fold_cost)


    def _constructAntSolution(self,ant):

        current_at_feature_subset = []

        featureSetIndex = []
        #for each feature index
        for j in range(self.size):

            #generate random number
            decision = np.random.rand()

            ## in the first iteration, fp has all values as 1 for the length of features

            if decision < self.fp[j] / 2.0:
                featureSetIndex.append(1)
                current_at_feature_subset.append(self.columns_list[j])
            else:
                featureSetIndex.append(0)

        # if no features, then just keep 0.5. Else, calculate the actual cost
        if sum(featureSetIndex) == 0:
            score = 0.5
        else:
            score = float(self._calculate_cost(current_at_feature_subset))

        #for the ant, assign score and feature indexes used.
        ant.val = score
        ant.subsets = deepcopy(featureSetIndex)

        return ant

    def _ApplyLocalSearch(self):

        maxSet = []

        if self.cost_function_improvement == 'decrease':
            maxScore = np.inf
        else:
            maxScore = 0

        #for each ant in the iteration
        for a in self.ants:

            if self.cost_function_improvement == 'decrease':
                if maxScore > a.val or (maxScore == a.val and (maxSet and sum(a.subsets) < sum(maxSet))):
                    maxScore = a.val
                    maxSet = a.subsets
            else:
                if maxScore < a.val or (maxScore == a.val and (maxSet and sum(a.subsets) < sum(maxSet))):
                    maxScore = a.val
                    maxSet = a.subsets

        ## After the search for best score is done and associated feature set binary vector is found,

        if self.cost_function_improvement == 'decrease':
            if self.topScore > maxScore or (maxScore == self.topScore and (self.result and sum(maxSet) < sum(self.result))):
                self.topScore = maxScore
                self.result = maxSet
        else:
            if self.topScore < maxScore or (maxScore == self.topScore and (self.result and sum(maxSet) < sum(self.result))):
                self.topScore = maxScore
                self.result = maxSet

        ##but return only local best result for current colony
        return maxSet, maxScore


    def _calc_update_param(self,topScore):


        sumResults = 0

        for a in self.ants:

            if sum(a.subsets) > 0:
                #value that is added is pehermone update coefficient divided by cost, for both current and top ant
                sumResults += (self.Q/a.val)

        return sumResults + (self.Q/topScore)


    def _UpdatePheromoneTrail(self,topSet, topScore):

        #get sum results
        sumResults = self._calc_update_param(topScore)

        #topSet is binary 1|0 feature vector. topScore is best score for entire colony

        for i,v in enumerate(topSet):

            #evaporate pheromene, based on formula  =   

            pheromone_at_index = self.fp[i]*self.evaporation_rate

            ## update pheromene trail

            if v == 1:
                pheromone_at_index += self.fp[i] + sumResults

            self.fp[i] = pheromone_at_index


    def GetBestFeatures(self):

        if self.cost_function_improvement == 'decrease':
            self.topScore = np.inf

        #get starting time
        start = time.time()

        ### For each iteratoin of ACO
        for iter_num in range(self.iterations):


            #for each ant
            for i in range(self.N_ants):
                #create new ant
                ant = Ant()
                #create the first initialization for ant
                ant = self._constructAntSolution(ant)
                self.ants.append(ant)

            ##for the iteration, after all colony of ants have been created

            topSet, topScore = self._ApplyLocalSearch()
            if self.print_iteration_result:
                print('Best combined performance on test and validation data for iteration '+str(iter_num)+': '+str(self.topScore))

            #give input the best feature binary 1|0 vector and best metric from the entire colony
            self._UpdatePheromoneTrail(topSet, topScore)
            self.ants = []

        ## After everything is done, just get the original name of feature.
        best_columns = []
        for indx in range(len(self.result)):
            if self.result[indx] == 1:
                best_columns.append(self.columns_list[indx])


        print('================= Best result:',self.topScore,'=================')
        print('================= Execution time in minutes:',(time.time()-start)//60,'=================')

        return best_columns

class Ant:
    def _init_(self):
        self.subsets = []
        self.val = 0



# Load your dataset from a CSV file
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/72_75_49_40.csv")

# Extract all column names except the target column
columns_list = data.columns.tolist()
columns_list.remove('target')  # Assuming 'target' is the name of your target column

# Assuming 'target' column is your target variable
X = data.drop(columns=['target'])  # Features
y = data['target']  # Target variable

# Split the data into training, testing, and validation sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_test, X_validation, y_test, y_validation = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Now you can create your data dictionary
data_dict = {
    0: {'x_train': X_train, 'y_train': y_train, 'x_test': X_test, 'y_test': y_test}
    # You can add more keys for cross-validation data if needed
}

# Initialize your model
model = RandomForestClassifier()

# Define your cost function
def cost_function(y_true, y_pred):
    return accuracy_score(y_true, y_pred)

# Instantiate the AntColonyOptimizationFS class
aco_fs = AntColonyOptimizationFS(columns_list=columns_list,
                                  data_dict=data_dict,
                                  model=model,
                                  cost_function=cost_function,
                                  x_validation_dataframe=X_validation,
                                  y_validation_dataframe=y_validation,
                                  use_validation_data=True,
                                  cost_function_improvement='increase',
                                  iterations=1,
                                  N_ants=100,
                                  print_iteration_result=True,
                                  evaporation_rate=0.9,
                                  Q=0.2)

# Get the best features
best_features = aco_fs.GetBestFeatures()

print("Best features selected:", best_features)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import time
from copy import deepcopy
import pandas as pd
import numpy as np

class AntColonyOptimizationFS:

    def _init_(self,columns_list,data_dict,model,cost_function,x_validation_dataframe=pd.DataFrame(),y_validation_dataframe=pd.DataFrame(),use_validation_data=True,cost_function_improvement='increase',average=None,iterations=100,N_ants=100, print_iteration_result = True,evaporation_rate=0.9,Q=0.2):
        self.columns_list=columns_list
        self.data_dict=data_dict
        self.x_validation_dataframe=x_validation_dataframe
        self.y_validation_dataframe=y_validation_dataframe
        self.use_validation_data=use_validation_data
        self.model=model
        self.cost_function=cost_function
        self.cost_function_improvement=cost_function_improvement
        self.average=average
        self.iterations = iterations
        self.N_ants = N_ants
        self.print_iteration_result = print_iteration_result
        self.evaporation_rate = evaporation_rate
        self.Q = Q

        self.fp = [1]*(len(columns_list))
        self.ants = []
        self.size = len(columns_list)
        self.topScore = 0
        self.result=[]

    def _calculate_cost(self,current_at_feature_subset):

        fold_cost = []

        for i in self.data_dict.keys():

            x_train=self.data_dict[i]['x_train'][current_at_feature_subset]
            y_train=self.data_dict[i]['y_train']

            x_test=self.data_dict[i]['x_test'][current_at_feature_subset]
            y_test=self.data_dict[i]['y_test']

            self.model.fit(x_train,y_train)

            y_test_predict=self.model.predict(x_test)

            if self.use_validation_data:
                y_validation_predict=self.model.predict(self.x_validation_dataframe[current_at_feature_subset])

            if self.average:
                fold_cost.append(self.cost_function(y_test,y_test_predict,average=self.average))
                fold_cost.append(self.cost_function(self.y_validation_dataframe,y_validation_predict,average=self.average))
            else:
                fold_cost.append(self.cost_function(y_test,y_test_predict))
                fold_cost.append(self.cost_function(self.y_validation_dataframe,y_validation_predict))

        return np.mean(fold_cost)


    def _constructAntSolution(self,ant):

        current_at_feature_subset = []

        featureSetIndex = []
        #for each feature index
        for j in range(self.size):

            #generate random number
            decision = np.random.rand()

            ## in the first iteration, fp has all values as 1 for the length of features

            if decision < self.fp[j] / 2.0:
                featureSetIndex.append(1)
                current_at_feature_subset.append(self.columns_list[j])
            else:
                featureSetIndex.append(0)

        # if no features, then just keep 0.5. Else, calculate the actual cost
        if sum(featureSetIndex) == 0:
            score = 0.5
        else:
            score = float(self._calculate_cost(current_at_feature_subset))

        #for the ant, assign score and feature indexes used.
        ant.val = score
        ant.subsets = deepcopy(featureSetIndex)

        return ant

    def _ApplyLocalSearch(self):

        maxSet = []

        if self.cost_function_improvement == 'decrease':
            maxScore = np.inf
        else:
            maxScore = 0

        #for each ant in the iteration
        for a in self.ants:

            if self.cost_function_improvement == 'decrease':
                if maxScore > a.val or (maxScore == a.val and (maxSet and sum(a.subsets) < sum(maxSet))):
                    maxScore = a.val
                    maxSet = a.subsets
            else:
                if maxScore < a.val or (maxScore == a.val and (maxSet and sum(a.subsets) < sum(maxSet))):
                    maxScore = a.val
                    maxSet = a.subsets

        ## After the search for best score is done and associated feature set binary vector is found,

        if self.cost_function_improvement == 'decrease':
            if self.topScore > maxScore or (maxScore == self.topScore and (self.result and sum(maxSet) < sum(self.result))):
                self.topScore = maxScore
                self.result = maxSet
        else:
            if self.topScore < maxScore or (maxScore == self.topScore and (self.result and sum(maxSet) < sum(self.result))):
                self.topScore = maxScore
                self.result = maxSet

        ##but return only local best result for current colony
        return maxSet, maxScore


    def _calc_update_param(self,topScore):


        sumResults = 0

        for a in self.ants:

            if sum(a.subsets) > 0:
                #value that is added is pehermone update coefficient divided by cost, for both current and top ant
                sumResults += (self.Q/a.val)

        return sumResults + (self.Q/topScore)


    def _UpdatePheromoneTrail(self,topSet, topScore):

        #get sum results
        sumResults = self._calc_update_param(topScore)

        #topSet is binary 1|0 feature vector. topScore is best score for entire colony

        for i,v in enumerate(topSet):

            #evaporate pheromene, based on formula  =   

            pheromone_at_index = self.fp[i]*self.evaporation_rate

            ## update pheromene trail

            if v == 1:
                pheromone_at_index += self.fp[i] + sumResults

            self.fp[i] = pheromone_at_index


    def GetBestFeatures(self):

        if self.cost_function_improvement == 'decrease':
            self.topScore = np.inf

        #get starting time
        start = time.time()

        ### For each iteratoin of ACO
        for iter_num in range(self.iterations):


            #for each ant
            for i in range(self.N_ants):
                #create new ant
                ant = Ant()
                #create the first initialization for ant
                ant = self._constructAntSolution(ant)
                self.ants.append(ant)

            ##for the iteration, after all colony of ants have been created

            topSet, topScore = self._ApplyLocalSearch()
            if self.print_iteration_result:
                print('Best combined performance on test and validation data for iteration '+str(iter_num)+': '+str(self.topScore))

            #give input the best feature binary 1|0 vector and best metric from the entire colony
            self._UpdatePheromoneTrail(topSet, topScore)
            self.ants = []

        ## After everything is done, just get the original name of feature.
        best_columns = []
        for indx in range(len(self.result)):
            if self.result[indx] == 1:
                best_columns.append(self.columns_list[indx])


        print('================= Best result:',self.topScore,'=================')
        print('================= Execution time in minutes:',(time.time()-start)//60,'=================')

        return best_columns

class Ant:
    def _init_(self):
        self.subsets = []
        self.val = 0



# Load your dataset from a CSV file
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/72_75_49_40.csv")

# Extract all column names except the target column
columns_list = data.columns.tolist()
columns_list.remove('target')  # Assuming 'target' is the name of your target column

# Assuming 'target' column is your target variable
X = data.drop(columns=['target'])  # Features
y = data['target']  # Target variable

# Split the data into training, testing, and validation sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_test, X_validation, y_test, y_validation = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Now you can create your data dictionary
data_dict = {
    0: {'x_train': X_train, 'y_train': y_train, 'x_test': X_test, 'y_test': y_test}
    # You can add more keys for cross-validation data if needed
}

# Initialize your model
model = RandomForestClassifier()

# Define your cost function
def cost_function(y_true, y_pred):
    return accuracy_score(y_true, y_pred)

# Instantiate the AntColonyOptimizationFS class
aco_fs = AntColonyOptimizationFS(columns_list=columns_list,
                                  data_dict=data_dict,
                                  model=model,
                                  cost_function=cost_function,
                                  x_validation_dataframe=X_validation,
                                  y_validation_dataframe=y_validation,
                                  use_validation_data=True,
                                  cost_function_improvement='increase',
                                  iterations=1,
                                  N_ants=100,
                                  print_iteration_result=True,
                                  evaporation_rate=0.9,
                                  Q=0.2)

# Get the best features
best_features = aco_fs.GetBestFeatures()

print("Best features selected:", best_features)

# import numpy as np
# import pandas as pd
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import accuracy_score
# from sklearn.model_selection import train_test_split
# import time

# class ParticleSwarmOptimizationFS:
#     def __init__(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=100, n_particles=30, w=0.5, c1=0.8, c2=0.9):
#         self.columns_list = columns_list
#         self.data_dict = data_dict
#         self.model = model
#         self.cost_function = cost_function
#         self.x_validation_dataframe = x_validation_dataframe
#         self.y_validation_dataframe = y_validation_dataframe
#         self.use_validation_data = use_validation_data
#         self.cost_function_improvement = cost_function_improvement
#         self.iterations = iterations
#         self.n_particles = n_particles
#         self.w = w  # inertia weight
#         self.c1 = c1  # cognitive parameter
#         self.c2 = c2  # social parameter

#         self.particles_position = np.random.rand(self.n_particles, len(columns_list)) > 0.5
#         self.particles_velocity = np.zeros((self.n_particles, len(columns_list)))
#         self.pbest_position = self.particles_position.copy()
#         self.gbest_position = self.particles_position[0].copy()
#         self.pbest_value = np.array([float('inf')] * self.n_particles)
#         self.gbest_value = float('inf')
#         self.dimension = len(columns_list)

#     def _calculate_cost(self, particle_position):
#         selected_features = [self.columns_list[i] for i in range(self.dimension) if particle_position[i]]
#         if not selected_features:  # If no features are selected, return a high cost
#             return float('inf')

#         fold_cost = []
#         for i in self.data_dict.keys():
#             x_train = self.data_dict[i]['x_train'][selected_features]
#             y_train = self.data_dict[i]['y_train']
#             x_test = self.data_dict[i]['x_test'][selected_features]
#             y_test = self.data_dict[i]['y_test']

#             self.model.fit(x_train, y_train)
#             y_test_predict = self.model.predict(x_test)

#             fold_cost.append(self.cost_function(y_test, y_test_predict))

#         if self.use_validation_data:
#             x_validation = self.x_validation_dataframe[selected_features]
#             y_validation_predict = self.model.predict(x_validation)
#             fold_cost.append(self.cost_function(self.y_validation_dataframe, y_validation_predict))

#         return np.mean(fold_cost)

#     def _update_velocity_and_position(self):
#         for i in range(self.n_particles):
#             r1, r2 = np.random.rand(), np.random.rand()
#             self.particles_velocity[i] = self.w * self.particles_velocity[i] + self.c1 * r1 * (self.pbest_position[i] - self.particles_position[i]) + self.c2 * r2 * (self.gbest_position - self.particles_position[i])
#             sigmoid_velocity = 1 / (1 + np.exp(-self.particles_velocity[i]))
#             self.particles_position[i] = np.random.rand(self.dimension) < sigmoid_velocity

#     def run_optimization(self):
#         for iteration in range(self.iterations):
#             for i in range(self.n_particles):
#                 current_cost = self._calculate_cost(self.particles_position[i])
#                 if current_cost < self.pbest_value[i]:
#                     self.pbest_position[i] = self.particles_position[i].copy()
#                     self.pbest_value[i] = current_cost

#                 if current_cost < self.gbest_value:
#                     self.gbest_position = self.particles_position[i].copy()
#                     self.gbest_value = current_cost

#             self._update_velocity_and_position()

#         best_features = [self.columns_list[i] for i in range(self.dimension) if self.gbest_position[i]]
#         return best_features, self.gbest_value

# # Example usage
# # Assume data loading and preparation is done here

# # Instantiate the PSO for feature selection
# pso_fs = ParticleSwarmOptimizationFS(columns_list=columns_list,
#                                      data_dict=data_dict,
#                                      model=RandomForestClassifier(),
#                                      cost_function=accuracy_score,
#                                      x_validation_dataframe=X_validation,
#                                      y_validation_dataframe=y_validation,
#                                      use_validation_data=True,
#                                      iterations=10,
#                                      n_particles=20)

# # Run PSO
# start_time = time.time()
# best_features, best_score = pso_fs.run_optimization()
# end_time = time.time()

# print("Best features selected:", best_features)
# print("Best score achieved:", best_score)
# print("Execution time:", (end_time - start_time), "seconds")

# class HybridOptimizationFS:
#     def __init__(self, columns_list, data_dict, model, cost_function, x_validation_dataframe=pd.DataFrame(), y_validation_dataframe=pd.DataFrame(), use_validation_data=True, cost_function_improvement='increase', iterations=10, aco_ants=50, pso_particles=20, aco_evaporation_rate=0.9, aco_Q=0.2, pso_w=0.5, pso_c1=0.8, pso_c2=0.9):
#         self.columns_list = columns_list
#         self.data_dict = data_dict
#         self.model = model
#         self.cost_function = cost_function
#         self.x_validation_dataframe = x_validation_dataframe
#         self.y_validation_dataframe = y_validation_dataframe
#         self.use_validation_data = use_validation_data
#         self.cost_function_improvement = cost_function_improvement
#         self.iterations = iterations
#         self.aco_ants = aco_ants
#         self.pso_particles = pso_particles
#         self.aco_evaporation_rate = aco_evaporation_rate
#         self.aco_Q = aco_Q
#         self.pso_w = pso_w
#         self.pso_c1 = pso_c1
#         self.pso_c2 = pso_c2

#         self.best_features = None
#         self.best_score = -np.inf

#     def _run_aco(self):
#         aco_fs = AntColonyOptimizationFS(columns_list=self.columns_list,
#                                           data_dict=self.data_dict,
#                                           model=self.model,
#                                           cost_function=self.cost_function,
#                                           x_validation_dataframe=self.x_validation_dataframe,
#                                           y_validation_dataframe=self.y_validation_dataframe,
#                                           use_validation_data=self.use_validation_data,
#                                           cost_function_improvement=self.cost_function_improvement,
#                                           iterations=1,  # Run ACO for 1 iteration only
#                                           N_ants=self.aco_ants,
#                                           print_iteration_result=False,
#                                           evaporation_rate=self.aco_evaporation_rate,
#                                           Q=self.aco_Q)
#         return aco_fs.GetBestFeatures()

#     def _run_pso(self):
#         pso_fs = ParticleSwarmOptimizationFS(columns_list=self.columns_list,
#                                               data_dict=self.data_dict,
#                                               model=self.model,
#                                               cost_function=self.cost_function,
#                                               x_validation_dataframe=self.x_validation_dataframe,
#                                               y_validation_dataframe=self.y_validation_dataframe,
#                                               use_validation_data=self.use_validation_data,
#                                               iterations=1,  # Run PSO for 1 iteration only
#                                               n_particles=self.pso_particles,
#                                               w=self.pso_w,
#                                               c1=self.pso_c1,
#                                               c2=self.pso_c2)
#         return pso_fs.run_optimization()

#     def run_hybrid_optimization(self):
#         for iteration in range(self.iterations):
#             if iteration % 2 == 0:  # Even iterations: Run ACO
#                 print("Running ACO iteration", iteration + 1)
#                 current_features = self._run_aco()
#             else:  # Odd iterations: Run PSO
#                 print("Running PSO iteration", iteration + 1)
#                 current_features, current_score = self._run_pso()

#             if current_score > self.best_score:
#                 self.best_features = current_features
#                 self.best_score = current_score

#         return self.best_features, self.best_score

# # Example usage
# # Assume data loading and preparation is done here

# # Instantiate the hybrid optimizer
# hybrid_optimizer = HybridOptimizationFS(columns_list=columns_list,
#                                         data_dict=data_dict,
#                                         model=RandomForestClassifier(),
#                                         cost_function=accuracy_score,
#                                         x_validation_dataframe=X_validation,
#                                         y_validation_dataframe=y_validation,
#                                         use_validation_data=True,
#                                         iterations=5,  # Total iterations for hybrid optimization
#                                         aco_ants=50,
#                                         pso_particles=20,
#                                         aco_evaporation_rate=0.9,
#                                         aco_Q=0.2,
#                                         pso_w=0.5,
#                                         pso_c1=0.8,
#                                         pso_c2=0.9)

# # Run hybrid optimization
# start_time = time.time()
# best_features, best_score = hybrid_optimizer.run_hybrid_optimization()
# end_time = time.time()

# print("Best features selected:", best_features)
# print("Best score achieved:", best_score)
# print("Execution time:", (end_time - start_time), "seconds")